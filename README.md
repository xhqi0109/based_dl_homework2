### 第二节 作业答案 
（小红书/B站: 小鹏Running & 文轩考研 ）

> **专注计算机考研复试**项目**辅导（深度学习方向）/ 保研全流程辅导**

---

### 神经网络的核心框架🌟🌟🌟🌟🌟
在神经网络的复杂体系里，数据、网络架构、损失函数、优化器、训练超参数以及评价指标，共同搭建起核心框架。

#### 数据：神经网络的 “基石”
数据作为神经网络的基础，其合理划分十分关键，通常分为训练集、验证集和测试集：
- **训练集**：用于模型的参数学习，让模型在大量数据中不断迭代优化。
- **验证集**：在训练过程中辅助调整超参数，避免模型过早陷入过拟合。
- **测试集**：用于评估最终模型的性能，检验模型在未知数据上的泛化能力。

#### 网络架构：神经网络的 “骨架”
网络架构是神经网络的主体框架，其中两个关键部分为：
- **批归一化（Batch Normalization）**：堪称神经网络的稳定器。它通过对每一批输入数据进行归一化处理，使数据分布更加稳定，有效加速模型收敛速度，减少训练时间，还能缓解梯度消失和梯度爆炸问题，增强模型的泛化能力。
- **激活函数**：赋予神经网络非线性能力。若没有激活函数，神经网络无论有多少层都只能实现线性变换，表达能力极为有限。而激活函数能让神经网络逼近任何非线性函数，极大地拓展了神经网络对复杂数据的处理能力。

#### 优化器：模型训练的 “引擎”
优化器是驱动模型参数更新的引擎，而梯度则在其中扮演着导航仪的角色：
- **梯度**：是一个向量，在多元函数中由函数在各个变量方向上的偏导数组成。它指示着函数在某一点处变化最快的方向和速率。基于梯度下降算法，模型沿着梯度的反方向更新参数，从而快速找到损失函数的最小值，提升模型性能。可以想象你在一座山上，要找一个最低的山谷。梯度就是告诉你在当前位置，朝哪个方向走能最快下山，以及下山速度有多快的一个指示。在模型训练里，梯度就是告诉模型参数往哪个方向调整，能让模型的损失（可以理解为模型预测的错误程度）下降得最快。我们一般是让模型沿着梯度的反方向去更新参数，这样就能让模型不断改进，越来越准。
- **优化器算法**：这就如同司机开车的方法。不同的优化器算法就像不同的开车策略，有的可能开得稳但速度慢，有的可能速度快但容易走偏。比如常见的随机梯度下降算法，就是每次用一部分数据来计算梯度，然后更新参数，像开车每次看一小段路来决定怎么开。还有Adagrad算法，会根据每个参数过去的梯度情况，自动调整每个参数的学习速度，就好像司机根据不同路段的情况，自动给不同的车轮调整速度一样。这些算法都是为了让模型能更高效地找到最好的参数，让模型训练得又快又好。

#### 训练超参数：模型性能的 “调节旋钮”
训练超参数如学习率、层数、神经元个数、batch size、正则化系数等，堪称模型性能的调节旋钮。这些超参数的细微变化都可能对模型性能产生显著影响：
- **学习率**：决定参数更新步长，过大导致模型不收敛甚至发散，过小则使训练缓慢。
- **层数和神经元个数**：决定模型复杂度，过多易引发过拟合。
- **batch size**：影响训练效率和稳定性。
- **正则化系数**：用于控制模型复杂度，防止过拟合。

#### 评价指标：衡量模型优劣的 “尺子”
评价指标是衡量模型优劣的关键标准：
- **分类任务**：常用准确率、精确率、召回率、F1值等指标。
- **回归任务**：多采用均方误差（MSE）、平均绝对误差（MAE）等指标。这些指标帮助我们清晰了解模型在不同任务上的表现，从而针对性地优化模型。

---

### 1. 激活函数的作用与选择🌟🌟🌟🌟
> [激活函数参考视频](https://www.bilibili.com/video/BV1qB4y1e7GJ?spm_id_from=333.788.videopod.sections&vd_source=0c681b71f162c7f69342fa800b58ec82) 

在神经网络的复杂架构中，激活函数起着举足轻重的作用，其合理运用与选择直接关乎神经网络的性能与效果。

#### 为什么神经网络中必须使用激活函数
激活函数是赋予神经网络非线性能力的关键要素。若神经网络缺失激活函数，无论其层数如何叠加，本质上都只是在执行线性变换。这意味着它的表达能力被局限在处理线性可分问题上，然而，现实世界里大量的问题呈现出非线性特征。激活函数的介入，使得神经网络能够逼近任意非线性函数，极大地拓展了其表达能力与学习能力，开启了处理复杂现实问题的大门。

从数学公式角度深入剖析，假设有一个简单的两层神经网络：
- **第一层**：
  $z_1 = W_1 x + b_1$
  其中，$x$ 代表输入数据，$W_1$ 是第一层的权重矩阵，负责对输入进行加权，$b_1$ 是第一层的偏置向量，用于调整输出值，$z_1$ 则是第一层的输出结果。
- **第二层**：
  $z_2 = W_2 z_1 + b_2$ 
  这里的 $W_2$ 是第二层的权重矩阵，$b_2$ 是第二层的偏置向量，$z_2$ 为第二层的最终输出。

将两层的计算过程合并，可以得到：
$z_2 = W_2 (W_1 x + b_1) + b_2 = (W_2 W_1) x + (W_2 b_1 + b_2)$
进一步简化为：
$z_2 = W' x + b'$
其中，新的权重矩阵 $W' = W_2 W_1$，新的偏置向量 $b' = W_2 b_1 + b_2$ 。由此可见，在没有激活函数的情况下，无论神经网络层数有多少，最终输出 $z_2$ 始终只是输入 $x$ 的线性变换，无法有效表达复杂的非线性关系。

而当在每一层后引入非线性激活函数 $f(\cdot)$ 时，情况发生了根本性转变。例如：
- 第一层输出变为：
  $a_1 = f(z_1) = f(W_1 x + b_1)$
- 第二层输出变为：
  $a_2 = f(z_2) = f(W_2 a_1 + b_2)$
此时，网络的最终输出 $a_2$ 不再是输入 $x$ 的简单线性变换，而是通过非线性激活函数 $f(\cdot)$ 融入了非线性因素，从而具备了处理复杂非线性问题的能力。

#### 输出层的激活函数怎么选择（掌握）
输出层激活函数的选择并非一概而论，而是紧密依赖于具体的任务类型。不同的任务需求决定了应当采用何种激活函数，以实现最佳的模型性能。
- **回归任务**：
    - **特点**：回归任务旨在预测连续的数值，输出值理论上可以是任意实数。
    - **激活函数**：
        - **线性激活函数**：通常情况下，回归任务可直接使用线性激活函数，即不额外施加激活函数，其公式为 $f(z) = z$ ，其中 $z$ 为神经网络的输出值。这种选择使得模型输出能够直接反映预测的数值。
        - **Sigmoid 激活函数**：当对输出范围有特定要求，需将其限制在 $[0, 1]$ 区间时，Sigmoid 激活函数成为合适之选。其公式为 $f(z) = \frac{1}{1 + e^{-z}}$ ，通过该函数的变换，神经网络的输出被映射到指定区间内。
- **分类任务**：
    - **特点**：分类任务要求输出能够清晰表示各个类别的概率分布，且所有类别概率之和必须为 1，以此来准确判断样本所属类别。
    - **激活函数**：Softmax 激活函数是分类任务的常用选择。其公式为 $f(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}$ ，其中 $z_i$ 是神经网络第 $i$ 个输出值，$n$ 代表类别总数，$f(z_i)$ 则精确表示第 $i$ 个类别的概率。通过 Softmax 函数，神经网络的输出被转化为符合概率分布要求的形式，便于进行分类决策。

---

### 2. 梯度的定义与作用🌟🌟🌟🌟🌟
[梯度下降算法参考视频](https://www.bilibili.com/video/BV1oY411N7Xz?spm_id_from=333.788.videopod.sections&vd_source=0c681b71f162c7f69342fa800b58ec82)

在机器学习和深度学习领域中，梯度是一个极为重要的概念，它与模型的优化过程紧密相连，对模型的性能起着关键作用。

- **梯度是什么?**
从数学定义来看，梯度是一个向量，尤其在多元函数的情境下，它由函数在各个变量方向上的偏导数所组成。用更直观的方式理解，假设我们站在一个二维的山坡上，那么梯度就像是一个指示牌，它所指向的方向正是山坡最陡峭的方向，而这个向量的大小则清晰地表示出坡度的陡峭程度。简而言之，梯度体现了函数在某一点处变化最为迅速的方向和速率。

- **它在模型优化中扮演什么角色?**
在模型优化的复杂流程里，梯度肩负着指引模型参数更新方向的重任。以广泛应用的梯度下降算法为基础，模型会沿着梯度的反方向来更新自身的参数。这背后的原理在于，梯度的反方向恰恰是函数值下降最为迅速的方向。通过这种方式，模型能够以更快的速度寻找到损失函数的最小值，进而实现性能的显著提升，让模型在训练过程中不断逼近最优解，更好地完成各种学习任务。

---

### 3. 优化器的作用与常见类型🌟🌟🌟🌟🌟
[反向传播算法参考视频](https://www.bilibili.com/video/BV1yG411x7Cc?spm_id_from=333.788.videopod.sections&vd_source=0c681b71f162c7f69342fa800b58ec82)

在深度学习模型的训练过程中，优化器的选择对模型的收敛速度和性能表现有着至关重要的影响。接下来，我们将详细探讨常见的优化器及其主要区别。

#### 常见的优化器有哪些
1. **SGD（随机梯度下降）**
    - **更新方式**：直接按照梯度的反方向更新参数。
    - **缺点**：收敛速度较慢，在训练过程中容易陷入局部最优解，并且其学习率是固定的，这可能导致训练过程不稳定，难以达到理想的收敛效果。
2. **带动量的 SGD（Momentum）**
    - **更新方式**：引入动量项，以此来加速收敛过程并减少训练过程中的震荡现象。
    - **优势**：特别适合处理高曲率或噪声较大的损失函数，能够借助动量的积累更好地跳出局部最优解，使模型的训练更加稳定和高效。
3. **Adagrad**
    - **更新方式**：Adagrad 能够根据每个参数的历史梯度自适应地调整学习率。
    - **特点**：对于频繁出现的特征，它会降低学习率；而对于稀疏特征，则会增加学习率。这种特性使得它在处理稀疏数据时表现出色，但随着训练的进行，学习率会逐渐减小，有可能导致训练过早停止。
4. **Adam（Adaptive Moment Estimation）**
    - **更新方式**：巧妙地结合了动量和自适应学习率的优点。
    - **优势**：收敛速度快，在大多数任务中都能展现出良好的性能，并且对超参数的选择相对鲁棒，不需要过多的调参工作就能取得不错的效果。

#### 它们的主要区别是什么
1. **随机梯度下降（SGD）**
    - **更新公式**：
      $
      \theta_{t + 1} = \theta_t - \eta \nabla_\theta J(\theta_t)
      $
      其中，$\theta_t$ 代表第 $t$ 次迭代时的模型参数；$\eta$ 是学习率，控制着参数更新的步长；$\nabla_\theta J(\theta_t)$ 则是损失函数 $J$ 对参数 $\theta$ 的梯度。
    - **特点**：计算过程简单直接，计算量相对较小，但由于其固定的学习率和简单的更新方式，容易陷入局部最优，并且收敛速度较慢。
2. **带动量的 SGD（Momentum）**
    - **更新公式**：
      $
      v_{t + 1} = \gamma v_t + \eta \nabla_\theta J(\theta_t)
      $
      $
      \theta_{t + 1} = \theta_t - v_{t + 1}
      $
      其中，$v_t$ 是动量项，表示之前梯度的累积；$\gamma$ 是动量系数，通常取值为 0.9 。
    - **特点**：动量项的引入使得在梯度方向一致时，能够加速梯度下降的过程，同时减少训练过程中的震荡，帮助模型更好地跳出局部最优解，尤其适用于处理高曲率或噪声较大的损失函数。
3. **Adagrad**
    - **更新公式**：
      $
      G_t = G_{t - 1} + (\nabla_\theta J(\theta_t))^2
      $
      $
      \theta_{t + 1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta J(\theta_t)
      $
      其中，$G_t$ 是历史梯度的平方和；$\epsilon$ 是一个很小的常数，如 $10^{-8}$ ，主要用于避免除零错误。
    - **特点**：通过对历史梯度平方和的计算，Adagrad 能够自适应地调整学习率。对于频繁出现的特征（梯度较大），学习率会降低；对于稀疏特征（梯度较小），学习率会增加。这使得它在处理稀疏数据时表现出色，但随着训练的推进，学习率逐渐减小，可能导致训练过早停止。
4. **Adam（Adaptive Moment Estimation）**
    - **更新公式**：
      $
      m_t = \beta_1 m_{t - 1} + (1 - \beta_1) \nabla_\theta J(\theta_t)
      $
      $
      v_t = \beta_2 v_{t - 1} + (1 - \beta_2) (\nabla_\theta J(\theta_t))^2
      $
      $
      \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
      $
      $
      \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
      $
      $
      \theta_{t + 1} = \theta_t - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
      $
      其中，$m_t$ 和 $v_t$ 分别是梯度的一阶矩（均值）和二阶矩（未中心化的方差）；$\beta_1$ 和 $\beta_2$ 是衰减率，通常取值为 0.9 和 0.999 ；$\hat{m}_t$ 和 $\hat{v}_t$ 是经过偏差校正后的值。
    - **特点**：Adam 优化器融合了动量和自适应学习率的优势，在大多数任务中都能快速收敛，并且对超参数的选择不敏感，具有较强的鲁棒性。

#### 主要区别及原因分析
| 优化器 | 主要区别 | 原因分析 |
| --- | --- | --- |
| **SGD** | 直接使用梯度更新参数，学习率固定。 | 简单但容易陷入局部最优，收敛速度慢。 |
| **Momentum** | 引入动量项，加速收敛并减少震荡。 | 动量项累积了历史梯度信息，帮助加速梯度方向一致的更新，减少震荡。 |
| **Adagrad** | 自适应调整学习率，对稀疏特征更友好。 | 历史梯度平方和导致学习率逐渐减小，适合稀疏数据，但可能导致训练过早停止。 |
| **Adam** | 结合动量和自适应学习率，收敛速度快，适合大多数任务。 | 动量和自适应学习率的结合使得 Adam 在大多数任务中表现良好，且对超参数选择不敏感。 |

#### 总结
- **SGD**：计算简单，适用于小规模数据集或理论研究场景，能够为研究提供基础的优化思路，但在实际大规模应用中存在一定局限性。
- **Momentum**：通过引入动量项，有效加速了收敛过程，特别适用于处理高曲率或噪声较大的损失函数，能够在复杂的训练环境中稳定地推动模型优化。
- **Adagrad**：在处理稀疏数据时具有独特优势，能够根据数据特征自适应调整学习率，但学习率逐渐减小的特性可能导致训练提前结束，需要谨慎使用。
- **Adam**：融合了多种优化策略的优点，收敛速度快且对超参数选择相对鲁棒，在大多数任务中都能表现出色，是目前应用较为广泛的优化器之一。

在实际
